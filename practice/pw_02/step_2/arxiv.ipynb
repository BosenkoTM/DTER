{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LIdOsOi_Zm1"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# –ë–õ–û–ö 1: –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö –ò –û–ë–™–Ø–í–õ–ï–ù–ò–ï –§–£–ù–ö–¶–ò–ô\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from IPython.display import display\n",
        "\n",
        "def get_industry_for_arxiv(primary_subject, title, abstract):\n",
        "    \"\"\"\n",
        "    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å—é –∏–∑ arXiv –ø–æ –æ—Ç—Ä–∞—Å–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (subject),\n",
        "    –∞ —Ç–∞–∫–∂–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏.\n",
        "    \"\"\"\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
        "    full_text = (str(title) + ' ' + str(abstract)).lower()\n",
        "\n",
        "    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (Primary_Subject) ‚Äî —ç—Ç–æ —Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±\n",
        "    if primary_subject:\n",
        "        subject = primary_subject.lower()\n",
        "        if subject.startswith('econ') or subject.startswith('q-fin'):\n",
        "            return '–≠–∫–æ–Ω–æ–º–∏–∫–∞'\n",
        "        if subject.startswith('q-bio'):\n",
        "            return '–ë–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞'\n",
        "        # –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ Computer Science, Statistics (ML), –∏ Electrical Engineering –æ—Ç–Ω–æ—Å–∏–º –∫ –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ\n",
        "        if subject.startswith(('cs.', 'stat.ml', 'eess.')):\n",
        "            return '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞'\n",
        "\n",
        "    # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –¥–∞–ª–∞ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –∏—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
        "    econ_keywords = [\n",
        "        'economic', 'finance', 'business', 'management', 'market',\n",
        "        'portfolio', 'gdp', 'monetary', 'trade'\n",
        "    ]\n",
        "    bio_keywords = [\n",
        "        'medical', 'health', 'disease', 'biodiversity', 'clinical',\n",
        "        'neuro', 'dementia', 'aneurysm', 'biology', 'bioauto'\n",
        "    ]\n",
        "\n",
        "    if any(keyword in full_text for keyword in econ_keywords):\n",
        "        return '–≠–∫–æ–Ω–æ–º–∏–∫–∞'\n",
        "    if any(keyword in full_text for keyword in bio_keywords):\n",
        "        return '–ë–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞'\n",
        "\n",
        "    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ, –Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –±—ã–ª–∞ –∏–∑ CS, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫—É\n",
        "    if primary_subject and primary_subject.lower().startswith('cs.'):\n",
        "        return '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞'\n",
        "\n",
        "    return '–ü—Ä–æ—á–µ–µ'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# –ë–õ–û–ö 2: –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –°–ö–†–ò–ü–¢–ê\n",
        "# ==============================================================================\n",
        "\n",
        "file_path = 'arxiv.txt'\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text_data = file.read()\n",
        "    print(f\"‚úÖ –§–∞–π–ª '{file_path}' —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª '{file_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "    print(\"‚û°Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤ —Ä–∞–±–æ—á—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É —Å–Ω–æ–≤–∞.\")\n",
        "    text_data = None\n",
        "\n",
        "if text_data:\n",
        "    # –†–∞–∑–¥–µ–ª—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É \"arXiv:\"\n",
        "    entries = re.split(r'(?=arXiv:)', text_data.strip())\n",
        "    parsed_data = []\n",
        "    print(f\"–ù–∞–π–¥–µ–Ω–æ {len(entries)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\")\n",
        "\n",
        "    for entry in entries:\n",
        "        if not entry.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π ---\n",
        "\n",
        "            # ID —Å—Ç–∞—Ç—å–∏ (e.g., 2510.07297)\n",
        "            arxiv_id_match = re.search(r'arXiv:(\\S+)', entry)\n",
        "            arxiv_id = arxiv_id_match.group(1) if arxiv_id_match else None\n",
        "\n",
        "            # –û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è (e.g., cs.AI)\n",
        "            subject_match = re.search(r'\\[.+?\\]\\s+([\\w\\.-]+)', entry)\n",
        "            primary_subject = subject_match.group(1) if subject_match else None\n",
        "\n",
        "            # –ì–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏\n",
        "            year_match = re.search(r'Submitted \\d+ \\w+, (\\d{4})', entry)\n",
        "            year = int(year_match.group(1)) if year_match else None\n",
        "\n",
        "            # –ê–≤—Ç–æ—Ä—ã\n",
        "            authors_match = re.search(r'Authors:\\s*(.+?)\\nAbstract:', entry, re.DOTALL)\n",
        "            authors = \" \".join(authors_match.group(1).split()) if authors_match else \"N/A\"\n",
        "\n",
        "            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º–µ–∂–¥—É —Å—Ç—Ä–æ–∫–æ–π —Å ID –∏ —Å—Ç—Ä–æ–∫–æ–π \"Authors:\")\n",
        "            title_match = re.search(r'cs\\.\\w+\\n(.*?)\\nAuthors:', entry, re.DOTALL) # –û–±—â–∏–π —Å–ª—É—á–∞–π\n",
        "            if not title_match: # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω\n",
        "                 title_match = re.search(r'\\n(.*?)\\nAuthors:', entry, re.DOTALL)\n",
        "            title = \" \".join(title_match.group(1).split()).strip() if title_match else \"N/A\"\n",
        "\n",
        "            # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)\n",
        "            abstract_match = re.search(r'Abstract:\\s*(.*?)\\s*‚ñΩ More', entry, re.DOTALL)\n",
        "            abstract = abstract_match.group(1).strip() if abstract_match else \"\"\n",
        "\n",
        "            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
        "            industry = get_industry_for_arxiv(primary_subject, title, abstract)\n",
        "\n",
        "            parsed_data.append({\n",
        "                'ID': arxiv_id,\n",
        "                'Title': title,\n",
        "                'Authors': authors,\n",
        "                'Year': year,\n",
        "                'Primary_Subject': primary_subject,\n",
        "                'Industry': industry,\n",
        "                'Source': 'arXiv'\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø–∏—Å–∏, –Ω–∞—á–∏–Ω–∞—é—â–µ–π—Å—è —Å '{entry[:30]}...': {e}\")\n",
        "\n",
        "    if parsed_data:\n",
        "        # –°–æ–∑–¥–∞–µ–º DataFrame\n",
        "        df = pd.DataFrame(parsed_data)\n",
        "\n",
        "        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
        "        df.dropna(subset=['ID', 'Title', 'Year'], inplace=True)\n",
        "        df['Year'] = df['Year'].astype(int)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª—ã\n",
        "        csv_filename = 'dataset2.csv'\n",
        "        excel_filename = 'dataset2.xlsx'\n",
        "        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
        "        df.to_excel(excel_filename, index=False)\n",
        "\n",
        "        print(f\"\\n‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª—ã '{csv_filename}' –∏ '{excel_filename}'.\")\n",
        "        print(\"\\nüìä –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –∏—Ç–æ–≥–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã:\")\n",
        "        display(df.head(10))\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è –í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\")"
      ],
      "metadata": {
        "id": "NaoU773u_fLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}